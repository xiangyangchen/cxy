#!/bin/bash

source /opt/qomolo/utils/qomolo_gcs_scripts/log/env

beijing_date_mark=""
if [[ ${QOMOLO_ROBOT_ID} == "" ]]; then
    echo -e "\033[31m必须配置QOMOLO_ROBOT_ID\033[0m"
    exit 0
fi
if [[ ${GCS_PASSWORD} == "" ]]; then
    echo -e "\033[31m必须配置地面站密码\033[0m"
    exit 0
fi
if [[ ${GCS_USERNAME} == "" ]]; then
    echo -e "\033[31m必须配置地面站用户名\033[0m"
    exit 0
fi
if [[ ${GCS_IP} == "" ]]; then
    echo -e "\033[31m必须配置地面站IP\033[0m"
    exit 0
fi
if [[ ${NAS_PATH} == "" ]]; then
    echo -e "\033[31m必须配置NAS路径\033[0m"
    exit 0
fi
if [[ ${CSV_FOLDER_NAME} == "" ]]; then
    echo -e "\033[31m必须配置CSV路径\033[0m"
    exit 0
fi
if [[ ${ODOM_FOLDER_NAME} == "" ]]; then
    echo -e "\033[31m必须配置odom路径\033[0m"
    exit 0
fi
disk_len=$(df -h | grep /data | grep -v docker | awk '{print $4}' | cut -f 1 -d "G")
if [[ "$disk_len" -lt "80" ]]; then
    echo "请停止操作，检查磁盘大小，容量已经低于80G"
    exit 0
fi

# 修改文件所有者
sudo chown -R nvidia /data/code/all_ws/ws

read -p "上传日志输入1:
录定位（凯爷+凌远+影姐）要的数据包输入2:
录感知和规控（兵兵+家园+许宁+嘉俊）要的数据包输入3:
录制编码器航向角标定数据包输入4：
上传定位坐船数据5：
定位误差分析csv数据上传6:
lcb定位csv7:
lcb所有.log .csv文件上传8:
" MODE

workspace="/data/code/all_ws/ws"
BJ_TIME=$(date -d "+8 hour" +%Y_%m_%d_%H%M%S)
function input_time_create_folder() {
    mkdir -p ${workspace}/logpush_tmp
    read -p "请输入问题发生时的北京时间:" PROBLEM_TIME
    echo $(date) >>/data/code/all_ws/ws/.user_input_time_log
    echo $PROBLEM_TIME >>/data/code/all_ws/ws/.user_input_time_log
    if [[ ${#PROBLEM_TIME} != 19 ]]; then
        echo "时间格式不对，无法执行，请按照格式来输入如2022-09-06 12:00:00"
        exit 0
    else
        UPLOAD_LOG_NAME=${HOSTNAME}_${BJ_TIME}_log_bag
        mkdir -p ${workspace}/logpush_tmp/${UPLOAD_LOG_NAME}/csv
        mkdir -p ${workspace}/logpush_tmp/${UPLOAD_LOG_NAME}/qlog
        mkdir -p ${workspace}/logpush_tmp/${UPLOAD_LOG_NAME}/supervisor_log
        mkdir -p ${workspace}/logpush_tmp/${UPLOAD_LOG_NAME}/localization_bag
        mkdir -p ${workspace}/logpush_tmp/${UPLOAD_LOG_NAME}/lidar
        mkdir -p ${workspace}/logpush_tmp/${UPLOAD_LOG_NAME}/vdr
        mkdir -p ${workspace}/logpush_tmp/${UPLOAD_LOG_NAME}/images
    fi
    TIMESTAMP=$(date -d "${PROBLEM_TIME}" +%s)
    TIMESTAMP_CHANGE=$(expr $TIMESTAMP - 28801) #8H
    SEARCH_TIME=$(date -d @$TIMESTAMP_CHANGE +%Y-%m-%d\ %H:%M:00)
}

#tools
function bag_gen_folder() {
    UPLOAD_LOG_NAME=${HOSTNAME}_${BJ_TIME}_$1
    mkdir -p /data/code/all_ws/ws/logpush_tmp/${UPLOAD_LOG_NAME}
    cd /data/code/all_ws/ws
}
function log_search_copy_pre() {
    beijing_date=$(date -d "8 hour" +%Y-%m-%d-%H-%M%S)
    cd /data/code/all_ws/ws/
    ls *.log* | xargs -I {} cp {} /data/code/all_ws/ws/igv_log/{}-${beijing_date}.log
}

# log search post
function log_search_copy_post() {
    if [[ -d /data/code/all_ws/ws/igv_log ]]; then
        rm /data/code/all_ws/ws/igv_log/*$beijing_date*
    else
        echo "igv_log not exist"
    fi
}

#keeper_error_event
function log_keeper_error_event_search_copy() {
    cd /data/code/all_ws/ws/igv_log && rsync -azv --progress keeper_error_event.log ${workspace}/logpush_tmp/${UPLOAD_LOG_NAME}/                    # 0926 old version   0926 Above delete
    cd /data/code/all_ws/ws/ && rsync -azv --progress keeper_error_event.log ${workspace}/logpush_tmp/${UPLOAD_LOG_NAME}/keeper_error_event.log.new # new version
}
function log_history_search_copy() {
    forward_list=$(find /data/code/all_ws/ws/igv_log/$1* -type f ! -newermt "${SEARCH_TIME}" | grep $1)
    if [[ $forward_list != "" ]]; then
        ls -t ${forward_list} | head -n 3 | xargs -I {} rsync -avz --progress {} /data/code/all_ws/ws/logpush_tmp/${UPLOAD_LOG_NAME}/supervisor_log
    fi
    sleep 1
    reverse_list=$(find /data/code/all_ws/ws/igv_log/$1* -type f -newermt "${SEARCH_TIME}" | grep $1)
    if [[ $reverse_list != "" ]]; then
        ls -rt ${reverse_list} | head -n 3 | xargs -I {} rsync -avz --progress {} /data/code/all_ws/ws/logpush_tmp/${UPLOAD_LOG_NAME}/supervisor_log
    fi
}
function csv_search_copy() {
    forward_list=$(find /data/code/all_ws/ws/${CSV_FOLDER_NAME}/$1* -type f ! -newermt "${SEARCH_TIME}" | grep $1)
    if [[ $forward_list != "" ]]; then
        ls -t ${forward_list} | head -n 2 | xargs -I {} rsync -avz --progress {} /data/code/all_ws/ws/logpush_tmp/${UPLOAD_LOG_NAME}/csv
    fi
    sleep 1
    reverse_list=$(find /data/code/all_ws/ws/${CSV_FOLDER_NAME}/$1* -type f -newermt "${SEARCH_TIME}" | grep $1)
    if [[ $reverse_list != "" ]]; then
        ls -rt ${reverse_list} | head -n 2 | xargs -I {} rsync -avz --progress {} /data/code/all_ws/ws/logpush_tmp/${UPLOAD_LOG_NAME}/csv
    fi
}
function high_csv_search_copy() {
    # TIMESTAMP=$(date -d "${SEARCH_TIME}" +%s)
    # TIMESTAMP_CHANGE=$(expr $TIMESTAMP - 1800) #30min
    # NEW_START_TIME=$(date -d @$TIMESTAMP_CHANGE +%Y-%m-%d\ %H:%M:00)
    # TIMESTAMP_CHANGE=$(expr $TIMESTAMP + 1800) #30min
    # NEW_END_TIME=$(date -d @$TIMESTAMP_CHANGE +%Y-%m-%d\ %H:%M:00)
    # find /data/code/all_ws/ws/${CSV_FOLDER_NAME} -name 'drivetask*.csv' -type f -newermt "${NEW_START_TIME}" ! -newermt "${NEW_END_TIME}" >/tmp/${TIMESTAMP}-test.txt
    # cp -v /tmp/${TIMESTAMP}-test.txt ~/test.txt
    # for i in $(cat /tmp/${TIMESTAMP}-test.txt); do
    #     cp -v  $i /data/code/all_ws/ws/logpush_tmp/${UPLOAD_LOG_NAME}/csv
    # done
    # if [[ -f /tmp/${TIMESTAMP}-test.txt ]]; then
    #     rm -r /tmp/${TIMESTAMP}-test.txt
    # fi
    forward_list=$(find /data/code/all_ws/ws/csv/$1* -type f ! -newermt "${SEARCH_TIME}" | grep $1)
    if [[ $forward_list != "" ]]; then
        ls -t ${forward_list} | head -n 10 | xargs -I {} rsync -avz --progress {} /data/code/all_ws/ws/logpush_tmp/${UPLOAD_LOG_NAME}/csv
    fi
    sleep 1
    reverse_list=$(find /data/code/all_ws/ws/csv/$1* -type f -newermt "${SEARCH_TIME}" | grep $1)
    if [[ $reverse_list != "" ]]; then
        ls -rt ${reverse_list} | head -n 10 | xargs -I {} rsync -avz --progress {} /data/code/all_ws/ws/logpush_tmp/${UPLOAD_LOG_NAME}/csv
    fi
}
function qlog_all_search_copy() {
    mkdir -p ${workspace}/logpush_tmp/${UPLOAD_LOG_NAME}/qlog/$1

    forward_list=$(find /data/code/all_ws/ws/qlog/$1* -type f ! -newermt "${SEARCH_TIME}" | grep $1 | grep $2)
    if [[ $forward_list != "" ]]; then
        ls -t $forward_list | grep $1 | head -n 2 | xargs -I {} rsync -avz --progress {} /data/code/all_ws/ws/logpush_tmp/${UPLOAD_LOG_NAME}/qlog/$1
    fi
    reverse_list=$(find /data/code/all_ws/ws/qlog/$1* -type f -newermt "${SEARCH_TIME}" | grep $1 | grep $2)
    if [[ $reverse_list != "" ]]; then
        ls -t $reverse_list | grep $1 | tail -n 2 | xargs -I {} rsync -avz --progress {} /data/code/all_ws/ws/logpush_tmp/${UPLOAD_LOG_NAME}/qlog/$1
    fi
}
function file_search_copy_odom() {
    if [[ -d ${ODOM_FOLDER_NAME} ]]; then
        cd ${ODOM_FOLDER_NAME}
        oldder_odom=$(find ./ -name "*.db3" ! -newermt "${SEARCH_TIME}")
        if [[ $oldder_odom != "" ]]; then
            ls -t $oldder_odom | head -n 5 | xargs -I {} rsync -avz --progress {} /data/code/all_ws/ws/logpush_tmp/${UPLOAD_LOG_NAME}/localization_bag/
        fi
        cd ${ODOM_FOLDER_NAME}
        newer_odom=$(find ./ -name "*.db3" -newermt "${SEARCH_TIME}")
        if [[ $newer_odom != "" ]]; then
            ls -rt $newer_odom | head -n 5 | xargs -I {} rsync -avz --progress {} /data/code/all_ws/ws/logpush_tmp/${UPLOAD_LOG_NAME}/localization_bag/
        fi
    else
        echo "odom path not exist"
    fi
}
function vdr_search_copy() {
    TIMESTAMP=$(date -d "${SEARCH_TIME}" +%s)
    TIMESTAMP_CHANGE=$(expr $TIMESTAMP - 1800) #30min
    NEW_START_TIME=$(date -d @$TIMESTAMP_CHANGE +%Y-%m-%d\ %H:%M:00)
    TIMESTAMP_CHANGE=$(expr $TIMESTAMP + 1800) #30min
    NEW_END_TIME=$(date -d @$TIMESTAMP_CHANGE +%Y-%m-%d\ %H:%M:00)

    #   If the number of files matched is greater than 5, shorten the match range to 10 minutes for copying
    find /data/code/all_ws/ws/csv/short_time/$1* -type d -newermt "${NEW_START_TIME}" ! -newermt "${NEW_END_TIME}" >/tmp/vdr-${TIMESTAMP}.log
    if [ $(cat /tmp/vdr-${TIMESTAMP}.log | wc -l) -lt 5 ]; then
        for i in $(cat /tmp/vdr-${TIMESTAMP}.log); do
            rsync -avz --progress $i /data/code/all_ws/ws/logpush_tmp/${UPLOAD_LOG_NAME}/vdr
        done
        if [[ -f /tmp/vdr-${TIMESTAMP}.log ]]; then
            rm -r /tmp/vdr-${TIMESTAMP}.log
        fi
    else
        TIMESTAMP=$(date -d "${SEARCH_TIME}" +%s)
        TIMESTAMP_CHANGE=$(expr $TIMESTAMP - 300) #10min
        NEW_START_TIME=$(date -d @$TIMESTAMP_CHANGE +%Y-%m-%d\ %H:%M:00)
        TIMESTAMP_CHANGE=$(expr $TIMESTAMP + 300) #10min
        NEW_END_TIME=$(date -d @$TIMESTAMP_CHANGE +%Y-%m-%d\ %H:%M:00)
        find /data/code/all_ws/ws/csv/short_time/$1* -type d -newermt "${NEW_START_TIME}" ! -newermt "${NEW_END_TIME}" >/tmp/$1-${TIMESTAMP}.log
        for i in $(cat /tmp/$1-${TIMESTAMP}.log); do
            rsync -avz --progress $i /data/code/all_ws/ws/logpush_tmp/${UPLOAD_LOG_NAME}/vdr
        done
        if [[ -f /tmp/$1-${TIMESTAMP}.log ]]; then
            rm -r /tmp/$1-${TIMESTAMP}.log
        fi
    fi
}

function folder_search_lidar() {
    find /data/key_log/lidar/* -type d ! -newermt "${SEARCH_TIME}" | tail -n 5 | xargs -I {} rsync -avz --progress {} /data/code/all_ws/ws/logpush_tmp/${UPLOAD_LOG_NAME}/lidar
    sleep 1
    find /data/key_log/lidar/* -type d -newermt "${SEARCH_TIME}" | head -n 5 | xargs -I {} rsync -avz --progress {} /data/code/all_ws/ws/logpush_tmp/${UPLOAD_LOG_NAME}/lidar

}
function folder_search_image() {
    find /data/key_log/image/$1* -type d ! -newermt "${SEARCH_TIME}" | grep $1 | tail -n 5 | xargs -I {} rsync -avz --progress {} /data/code/all_ws/ws/logpush_tmp/${UPLOAD_LOG_NAME}/images
    sleep 1
    find /data/key_log/image/$1* -type d -newermt "${SEARCH_TIME}" | grep $1 | head -n 5 | xargs -I {} rsync -avz --progress {} /data/code/all_ws/ws/logpush_tmp/${UPLOAD_LOG_NAME}/images
}

## other need
function localization_bag() {
    UPLOAD_LOG_NAME=${HOSTNAME}_$(date -d "8 hour" +%Y-%m-%d-%H%M)_localization_collect
    mkdir -p /data/code/all_ws/ws/logpush_tmp/${UPLOAD_LOG_NAME}/odom_bag
    if [[ ${#START_TIME} != 19 ]]; then
        echo "时间格式不对，无法执行，请按照格式来输入如2022-02-22 12:00:00"
        exit 0
    fi
    if [[ ${#END_TIME} != 19 ]]; then
        echo "时间格式不对，无法执行，请按照格式来输入如2022-02-22 12:00:00"
        exit 0
    fi
    cd ${ODOM_FOLDER_NAME}
    find ./ -name '*.db3' -type f -newermt "$START_TIME" ! -newermt "$END_TIME" | xargs -I {} rsync -avz --progress {} /data/code/all_ws/ws/logpush_tmp/${UPLOAD_LOG_NAME}/odom_bag/
}
function localization_log() {
    UPLOAD_LOG_NAME=${HOSTNAME}_$(date -d "8 hour" +%Y-%m-%d-%H%M)_localization_collect
    mkdir -p /data/code/all_ws/ws/logpush_tmp/${UPLOAD_LOG_NAME}/localization_check_log
    mkdir -p /data/code/all_ws/ws/logpush_tmp/${UPLOAD_LOG_NAME}/keep_log
    cd /data/code/all_ws/ws/igv_log
    start_time=$(date -d "$START_TIME" +%s)
    start_utc=$(expr $start_time - 28000)
    start_bj=$(date -d @$start_utc "+%Y-%m-%d %H:%M:%S")
    end_time=$(date -d "$END_TIME" +%s)
    end_utc=$(expr $end_time - 28000)
    end_bj=$(date -d @$end_utc "+%Y-%m-%d %H:%M:%S")
    find ./ -name 'localization_check*' -type f -newermt "$start_bj" ! -newermt "$end_bj" | xargs -I {} rsync -avz --progress {} /data/code/all_ws/ws/logpush_tmp/${UPLOAD_LOG_NAME}/localization_check_log/
    sleep 1
    find /data/code/all_ws/ws/igv_log -name "keep*" -type f -newermt "$start_bj" ! -newermt "$end_bj" | xargs -I {} rsync -avz --progress {} /data/code/all_ws/ws/logpush_tmp/${UPLOAD_LOG_NAME}/keep_log/
}

function time_select() {
    INPUT=""
    read -p "默认提取昨天的csv数据（回车即可），
如果需要指定日期请输入 （y）" INPUT
    if [[ "${INPUT}" == "" ]]; then
        echo
    elif [[ "${INPUT}" == "y" ]]; then
        read -p "开始时间： " START_TIME
        read -p "结束时间:  " END_TIME
        if [[ ${#START_TIME} != 19 ]] && [[ ${#END_TIME} != 19 ]]; then
            echo "时间格式不对，无法执行，请按照格式来输入如2023-03-06 22:00:00"
            exit 0
        else
            start_time=$(date -d "$START_TIME" +%s)
            start_utc=$(expr $start_time - 28000)
            start_obj=$(date -d @$start_utc "+%Y-%m-%d %H:%M:%S")
            end_time=$(date -d "$END_TIME" +%s)
            end_utc=$(expr $end_time - 28000)
            end_obj=$(date -d @$end_utc "+%Y-%m-%d %H:%M:%S")
            echo "提取文件开始  >>开始时间为： " ${START_TIME} " >>结束时间为:  " ${END_TIME}
        fi
    fi
}

function localization_csv_search() {

    TIMESTAMP=$(date -d "${SEARCH_TIME}" +%s)
    SELECTTIME=$(date -d "yesterday" '+%Y-%m-%d')
    STARTTIME=$(date -d "${SELECTTIME} 00:00:00 9 hours ago" "+%Y-%m-%-d %H:%M:%S")
    SELECTTIME=$(date -d "yesterday" '+%Y-%m-%d')
    ENDTIME=$(date -d "${SELECTTIME} 17:00:00" "+%Y-%m-%-d %H:%M:%S")
    if [ -d /data/code/all_ws/ws/logpush_tmp/${UPLOAD_LOG_NAME}/localization_csv ]; then
        echo
    else
        sudo mkdir -p /data/code/all_ws/ws/logpush_tmp/${UPLOAD_LOG_NAME}/localization_csv
    fi
    if [[ ${INPUT} == "" ]]; then
        if [ -d /data/code/all_ws/ws/csv/ ]; then
            if [[ ${STARTTIME} != "" ]] && [[ ${ENDTIME} != "" ]]; then
                echo ${STARTTIME} ${ENDTIME}
                cd /data/code/all_ws/ws/csv/ &&
                    find /data/code/all_ws/ws/csv/ -name "$1*" -type f -newermt "${STARTTIME}" ! -newermt "${ENDTIME}" | grep $1 >/tmp/${TIMESTAMP}-localization_csv.txt
            else
                echo "Not time"
            fi
            for i in $(cat /tmp/${TIMESTAMP}-localization_csv.txt); do
                rsync -avz --progress $i /data/code/all_ws/ws/logpush_tmp/${UPLOAD_LOG_NAME}/localization_csv/
            done
            if [[ -f /tmp/${TIMESTAMP}-localization_csv.txt ]]; then
                rm -r /tmp/${TIMESTAMP}-localization_csv.txt
            fi
        fi
    elif [[ "${INPUT}" == "y" ]]; then
        if [ -d /data/code/all_ws/ws/csv/ ]; then
            echo ${start_time} ${end_time}
            cd /data/code/all_ws/ws/csv/ &&
                find /data/code/all_ws/ws/csv/ -name "$1*" -type f -newermt "${start_obj}" ! -newermt "${end_obj}" | grep $1 >/tmp/${TIMESTAMP}-localization_csv.txt
        fi
        for i in $(cat /tmp/${TIMESTAMP}-localization_csv.txt); do
            rsync -avz --progress $i /data/code/all_ws/ws/logpush_tmp/${UPLOAD_LOG_NAME}/localization_csv/
        done
        if [[ -f /tmp/${TIMESTAMP}-localization_csv.txt ]]; then
            rm -r /tmp/${TIMESTAMP}-localization_csv.txt
        fi
    fi
}

function lcb_localization_csv()
{
    yesterdayTime=$(date -d "$(date -d "yesterday" '+%Y-%m-%d') 02:00:00" "+%Y-%m-%-d %H:%M:%S")
    SEARCH_TIME=$(date -d @$(date +%s) +%Y-%m-%d\ %H:%M:00)
    if [ -d /data/code/all_ws/ws/csv/ ]; then
        echo ${yesterdayTime} ${SEARCH_TIME}
        cd /data/code/all_ws/ws/csv/ &&
            find /data/code/all_ws/ws/csv/ -name "$1*" -type f -newermt "${yesterdayTime}" ! -newermt "${SEARCH_TIME}" | grep $1 >/tmp/${TIMESTAMP}-localization_csv.txt
    fi
    for i in $(cat /tmp/${TIMESTAMP}-localization_csv.txt); do
        rsync -avz --progress $i /data/code/all_ws/ws/logpush_tmp/${UPLOAD_LOG_NAME}/localization_csv/
    done
    if [[ -f /tmp/${TIMESTAMP}-localization_csv.txt ]]; then
        rm -r /tmp/${TIMESTAMP}-localization_csv.txt
    fi
    
}

# search all log for /data/code/all_ws/ws/igv_log/ 
function lcb_supervisor_log()
{
    LPATH="igv_log"
    DIR_NAME="supervisor_log"

    yesterdayTime=$(date -d "$(date -d "yesterday" '+%Y-%m-%d') 02:00:00" "+%Y-%m-%-d %H:%M:%S")
    SEARCH_TIME=$(date -d @$(date +%s) +%Y-%m-%d\ %H:%M:00)

    if [ -d /data/code/all_ws/ws/${LPATH}/ ]; then
        echo ${yesterdayTime} ${SEARCH_TIME}
        cd /data/code/all_ws/ws/${LPATH}/ &&
            find /data/code/all_ws/ws/${LPATH}/ -type f -newermt "${yesterdayTime}" ! -newermt "${SEARCH_TIME}"  >/tmp/${TIMESTAMP}-${DIR_NAME}.txt
    fi
    for i in $(cat /tmp/${TIMESTAMP}-${DIR_NAME}.txt); do
        rsync -avz --progress $i /data/code/all_ws/ws/logpush_tmp/${UPLOAD_LOG_NAME}/${DIR_NAME}/
    done
    if [[ -f /tmp/${TIMESTAMP}-${DIR_NAME}.txt ]]; then
        rm -r /tmp/${TIMESTAMP}-${DIR_NAME}.txt
    fi
    
}

# search all csv for /data/code/all_ws/ws/csv/
function lcb_supervisor_csv()
{
    CPATH="csv"
    DIR_NAME="supervisor_csv"

    yesterdayTime=$(date -d "$(date -d "yesterday" '+%Y-%m-%d') 02:00:00" "+%Y-%m-%-d %H:%M:%S")
    SEARCH_TIME=$(date -d @$(date +%s) +%Y-%m-%d\ %H:%M:00)

    if [ -d /data/code/all_ws/ws/${CPATH}/ ]; then
        echo ${yesterdayTime} ${SEARCH_TIME}
        cd /data/code/all_ws/ws/${CPATH}/ &&
            find /data/code/all_ws/ws/${CPATH}/ -type f -newermt "${yesterdayTime}" ! -newermt "${SEARCH_TIME}" -maxdepth 1  >/tmp/${TIMESTAMP}-${DIR_NAME}.txt
    fi
    for i in $(cat /tmp/${TIMESTAMP}-${DIR_NAME}.txt); do
        rsync -avz --progress $i /data/code/all_ws/ws/logpush_tmp/${UPLOAD_LOG_NAME}/${DIR_NAME}/
    done
    if [[ -f /tmp/${TIMESTAMP}-${DIR_NAME}.txt ]]; then
        rm -r /tmp/${TIMESTAMP}-${DIR_NAME}.txt
    fi
    
}

###############################################################
function search_lidar_cps_csv() {
    find /data/code/all_ws/ws/key_log/lidar_cps/$1* -type f ! -newermt "${SEARCH_TIME}" | grep $1 | tail -n 3 | xargs -I {} rsync -avz --progress {} /data/code/all_ws/ws/logpush_tmp/${UPLOAD_LOG_NAME}/csv
    sleep 1
    find /data/code/all_ws/ws/key_log/lidar_cps/$1* -type f -newermt "${SEARCH_TIME}" | grep $1 | head -n 3 | xargs -I {} rsync -avz --progress {} /data/code/all_ws/ws/logpush_tmp/${UPLOAD_LOG_NAME}/csv
}

function compress_only() {
    rsync -avz --progress /opt/qomolo/qpilot/qpilot.repos /data/code/all_ws/ws/logpush_tmp/${UPLOAD_LOG_NAME}/
    echo "=========================================以下数据正在被压缩========================================="
    cd /data/code/all_ws/ws/logpush_tmp/
   echo nvidia | sudo -S tar -zvcf ${UPLOAD_LOG_NAME}.tar.gz ${UPLOAD_LOG_NAME}
   echo "压缩完毕，文件名："
   echo /data/code/all_ws/ws/logpush_tmp/${UPLOAD_LOG_NAME}.tar.gz
   sudo rm -r /data/code/all_ws/ws/logpush_tmp/${UPLOAD_LOG_NAME}/
}


function upload_to_nas() {
    rsync -avz --progress /opt/qomolo/qpilot/qpilot.repos /data/code/all_ws/ws/logpush_tmp/${UPLOAD_LOG_NAME}/
    echo "=========================================以下数据正在被压缩========================================="
    cd /data/code/all_ws/ws/logpush_tmp/
    echo nvidia | sudo -S nice -n 19 tar -zvcf ${UPLOAD_LOG_NAME}.tar.gz ${UPLOAD_LOG_NAME}
    mkdir -p /data/code/all_ws/ws/logpush_nas/
    cp /data/code/all_ws/ws/logpush_tmp/${UPLOAD_LOG_NAME}.tar.gz /data/code/all_ws/ws/logpush_nas/
    YEAR=$(date -d "8 hour" +%Y)
    MONTH=$(date -d "8 hour" +%m)
    DAY=$(date -d "8 hour" +%d)
    UPLOAD_PATH=${NAS_PATH}/${YEAR}/${MONTH}/${DAY}/${HOSTNAME}
    source /opt/qomolo/utils/qomolo_gcs_scripts/log/env
    sudo python3 /opt/qomolo/utils/qomolo_gcs_scripts/logpush_nas/manual_upload.py /data/code/all_ws/ws/logpush_nas/${UPLOAD_LOG_NAME}.tar.gz ${NAS_PATH} >/dev/zero
    echo "========================================请把以下路径粘贴到issue==================================="
    echo ${UPLOAD_PATH}/${UPLOAD_LOG_NAME}.tar.gz
    echo "=================================================================================================="
}

function upload_to_gcs() {
    rsync -avz --progress /opt/qomolo/qpilot/qpilot.repos /data/code/all_ws/ws/logpush_tmp/${UPLOAD_LOG_NAME}/
    echo "=========================================以下数据正在被压缩========================================="
    cd /data/code/all_ws/ws/logpush_tmp/
    echo nvidia | sudo -S nice -n 19 tar -zvcf ${UPLOAD_LOG_NAME}.tar.gz ${UPLOAD_LOG_NAME}
    YEAR=$(date -d "8 hour" +%Y)
    MONTH=$(date -d "8 hour" +%m)
    DAY=$(date -d "8 hour" +%d)
    UPLOAD_PATH=${NAS_PATH}/${YEAR}/${MONTH}/${DAY}/${HOSTNAME}
    echo "===========================================数据压缩已完成=========================================="
    echo "数据正在传输中......"
    sshpass -p ${GCS_PASSWORD} scp -l 10000 -o ServerAliveInterval=30 -o "StrictHostKeyChecking no" ${UPLOAD_LOG_NAME}.tar.gz ${GCS_USERNAME}@${GCS_IP}:/key_log/key_log/
    if [ $? != 0 ]; then
        echo -e "\033[031m数据因为网络原因传输失败，请联系管理员\033[0m"
        exit 0
    else
        echo "========================================请把以下路径粘贴到issue==================================="
        echo ${UPLOAD_PATH}/${UPLOAD_LOG_NAME}.tar.gz
        echo "=================================================================================================="
    fi
}

function main() {
    input_time_create_folder
    log_search_copy_pre
    sleep 5
    #log
    log_keeper_error_event_search_copy
    log_history_search_copy function_control
    log_history_search_copy mqtt_adaptor
    log_history_search_copy mono_lane_tracker_ros2
    log_history_search_copy canbus
    log_history_search_copy local_plan
    log_history_search_copy localization_checker
    log_history_search_copy vehicle_control
    log_history_search_copy qomolo_assembly
    log_history_search_copy alignment_planner
    log_history_search_copy agent
    log_history_search_copy keeper
    log_history_search_copy landmark_localizer
    log_history_search_copy lidar_estop
    log_history_search_copy lidar_preprocess
    log_history_search_copy localization_logger
    log_history_search_copy vehicle_data_recorder
    log_history_search_copy wheel_odom
    log_history_search_copy localization_adaptor
    log_history_search_copy gnss_driver
    log_history_search_copy gnss_processor
    log_history_search_copy hesai_lidar_4in1
    log_history_search_copy ros2_http
    log_history_search_copy http_bridge
    log_history_search_copy vehicle.
    log_history_search_copy lidar_config_check
    log_history_search_copy fusion
    log_history_search_copy mono_lane_tracker
    log_history_search_copy lidar_cps_alignment
    log_history_search_copy lstr
    log_history_search_copy qtruck_adaptor
    #csv
    high_csv_search_copy trajectory_conversion
    high_csv_search_copy drivetask
    csv_search_copy velocity_position_longitudinal
    csv_search_copy alignment
    csv_search_copy control_20 # V2.5
    csv_search_copy igv_nlfb_lat_controlle
    csv_search_copy igv_speed_lon_controller
    csv_search_copy inposition_log
    csv_search_copy lattice_planner
    csv_search_copy planning20 # V2.5
    csv_search_copy planning_trajectorys
    csv_search_copy planning_vehicle_state
    csv_search_copy localization2_reflector
    csv_search_copy localization2_wall
    csv_search_copy localization2_qc_match_pose
    csv_search_copy localization2_pillar
    csv_search_copy localization2_optimized_pose
    csv_search_copy localization2_odom_velocity
    csv_search_copy localization2_magnetic_nail
    csv_search_copy localization2_localization_odom
    csv_search_copy localization2_lidar_lane
    csv_search_copy localization2_gnss_verification
    csv_search_copy localization2_gnss_pose
    csv_search_copy localization2_feature_graph
    csv_search_copy localization2_diamond
    csv_search_copy localization2_dashed_line
    csv_search_copy localization2_correct_pose_by_wall
    csv_search_copy localization2_correct_pose_by_lane
    csv_search_copy localization2_corner
    csv_search_copy localization2_container_pose
    csv_search_copy localization2_beacon
    csv_search_copy route_task	
    #for i in ERROR WARNING INFO; do
    #    #qlog
    #    qlog_all_search_copy agent $i
    #    qlog_all_search_copy alignment_planner $i
    #    qlog_all_search_copy vdr $i
    #    qlog_all_search_copy planning $i
    #    qlog_all_search_copy perception $i
    #    qlog_all_search_copy canbus $i
    #    qlog_all_search_copy control $i
    #    qlog_all_search_copy keeper $i
    #    qlog_all_search_copy function_control $i
    #    qlog_all_search_copy localization $i
    #    qlog_all_search_copy planner $i
    #    qlog_all_search_copy vehicle $i
    #done
    #odom rosbag
    file_search_copy_odom
    #vdr
    #vdr_search_copy lidar
    #vdr_search_copy localization
    #vdr_search_copy local_plan_vdr
    #vdr_search_copy aeb_vdr
    #vdr_search_copy planning
    # localization_lidar
    #images
    #folder_search_image 202 # ^202
    #folder_search_lidar
    log_search_copy_post

    #
    search_lidar_cps_csv lidarcps
}
#upload
case "$MODE" in
1*)
    #天津
    if [[ "${HOSTNAME}" =~ ^TJ_IGV.* ]] || [[ "${HOSTNAME}" =~ ^ck.* ]] || [[ "${HOSTNAME}" =~ ^CN_CK.* ]] || [[ "${HOSTNAME}" =~ ^hr.* ]] || [[ "${HOSTNAME}" =~ ^pd.* ]]; then
        main
        #upload_to_gcs
    elif [[ "${HOSTNAME}" =~ ^wh.* ]] || [[ "${HOSTNAME}" =~ ^dl.* ]] || [[ "${HOSTNAME}" =~ ^eh.* ]] || [[ "${HOSTNAME}" =~ ^pd.* ]] || [[ "${HOSTNAME}" =~ ^hd.* ]] || [[ "${HOSTNAME}" =~ ^qz.* ]] || [[ "${HOSTNAME}" =~ ^tug.* ]] || [[ "${HOSTNAME}" =~ ^hkg.* ]] || [[ "${HOSTNAME}" =~ ^lcb.* ]] || [[ "${HOSTNAME}" =~ ^skq.* ]]; then
        main
	    compress_only
        #upload_to_nas
    else
        echo "所在项目不支持"
    fi
    ;;
# 录定位数据包
2*)
    bag_gen_folder localization_bag
    docker exec -it qpilot bash -c "source /opt/qomolo/qpilot/setup.bash &&  ros2 bag record /clock /${QOMOLO_ROBOT_ID}/odom /${QOMOLO_ROBOT_ID}/gnss/odom /${QOMOLO_ROBOT_ID}/localization/odom /${QOMOLO_ROBOT_ID}/full_pointcloud"
    bag_folder=$(ls rosbag2* -d -t | head -n 1)
    echo "已经录制成功的数据是这份： /data/code/all_ws/ws/"$bag_folder
    rsync -avz --progress /data/code/all_ws/ws/$bag_folder /data/code/all_ws/ws/logpush_tmp/${UPLOAD_LOG_NAME}/
    if [[ "${HOSTNAME}" =~ ^wh.* ]] || [[ "${HOSTNAME}" =~ ^dl.* ]] || [[ "${HOSTNAME}" =~ ^eh.* ]] || [[ "${HOSTNAME}" =~ ^pd.* ]] || [[ "${HOSTNAME}" =~ ^hd.* ]] || [[ "${HOSTNAME}" =~ ^qz.* ]] || [[ "${HOSTNAME}" =~ ^tug.* ]] || [[ "${HOSTNAME}" =~ ^hkg.* ]] || [[ "${HOSTNAME}" =~ ^lcb.* ]] || [[ "${HOSTNAME}" =~ ^skq.* ]]; then
        upload_to_nas
    else
        upload_to_gcs
    fi
    ;;
# 录感知数据包
3*)
    bag_gen_folder perception_bag
    docker exec -it qpilot bash -c "source /opt/qomolo/qpilot/setup.bash && ros2 bag record /${QOMOLO_ROBOT_ID}/tf /${QOMOLO_ROBOT_ID}/tf_static /${QOMOLO_ROBOT_ID}/lidar_estop_viz \
                        /${QOMOLO_ROBOT_ID}/odom /${QOMOLO_ROBOT_ID}/gnss/odom /${QOMOLO_ROBOT_ID}/pandar/front_left /${QOMOLO_ROBOT_ID}/pandar/front_right /${QOMOLO_ROBOT_ID}/pandar/rear_left \
                        /${QOMOLO_ROBOT_ID}/pandar/rear_right  /${QOMOLO_ROBOT_ID}/pandar/front_mid /${QOMOLO_ROBOT_ID}/pandar/rear_mid /rslidar_points/front /rslidar_points/rear \
                        /${QOMOLO_ROBOT_ID}/lidar_preprocess/wheelbox /${QOMOLO_ROBOT_ID}/filtered_pointcloud /${QOMOLO_ROBOT_ID}/localization/odom /${QOMOLO_ROBOT_ID}/local_plan_new \
                        /${QOMOLO_ROBOT_ID}/planning_debug /${QOMOLO_ROBOT_ID}/chassis_state_feedback /${QOMOLO_ROBOT_ID}/aeb_debug"
    bag_folder=$(ls rosbag2* -d -t | head -n 1)
    echo "已经录制成功的数据是这份： /data/code/all_ws/ws/"$bag_folder
    rsync -avz --progress /data/code/all_ws/ws/$bag_folder /data/code/all_ws/ws/logpush_tmp/${UPLOAD_LOG_NAME}/
    if [[ "${HOSTNAME}" =~ ^wh.* ]] || [[ "${HOSTNAME}" =~ ^dl.* ]] || [[ "${HOSTNAME}" =~ ^eh.* ]] || [[ "${HOSTNAME}" =~ ^pd.* ]] || [[ "${HOSTNAME}" =~ ^hd.* ]] || [[ "${HOSTNAME}" =~ ^qz.* ]] || [[ "${HOSTNAME}" =~ ^tug.* ]] || [[ "${HOSTNAME}" =~ ^hkg.* ]] || [[ "${HOSTNAME}" =~ ^lcb.* ]] || [[ "${HOSTNAME}" =~ ^skq.* ]]; then
        upload_to_nas
    else
        upload_to_gcs
    fi
    ;;
4*)
    bag_gen_folder sensor_bag
    docker exec -it qpilot bash -c "source /opt/qomolo/qpilot/setup.bash &&  ros2 bag record /clock /${QOMOLO_ROBOT_ID}/odom /${QOMOLO_ROBOT_ID}/gnss/odom"
    bag_folder=$(ls rosbag2* -d -t | head -n 1)
    echo "已经录制成功的数据是这份： /data/code/all_ws/ws/"$bag_folder
    rsync -avz --progress /data/code/all_ws/ws/$bag_folder /data/code/all_ws/ws/logpush_tmp/${UPLOAD_LOG_NAME}/
    if [[ "${HOSTNAME}" =~ ^wh.* ]] || [[ "${HOSTNAME}" =~ ^dl.* ]] || [[ "${HOSTNAME}" =~ ^eh.* ]] || [[ "${HOSTNAME}" =~ ^pd.* ]] || [[ "${HOSTNAME}" =~ ^hd.* ]] || [[ "${HOSTNAME}" =~ ^qz.* ]] || [[ "${HOSTNAME}" =~ ^tug.* ]] || [[ "${HOSTNAME}" =~ ^hkg.* ]] || [[ "${HOSTNAME}" =~ ^lcb.* ]] || [[ "${HOSTNAME}" =~ ^skq.* ]]; then
        upload_to_nas
    else
        upload_to_gcs
    fi
    ;;
5*)
    read -p "输入坐船开始时间：" START_TIME
    read -p "输入坐船结束时间：" END_TIME
    read -p "是否需要rosbag y/n : " number
    if [[ "$number" = "y" ]] || [[ "$number" = "Y" ]] || [[ "$number" = "" ]] || [[ "$number" = "yes" ]]; then
        localization_bag
        keeper_error_event
        localization_log
        upload_to_gcs
    else
        localization_log
        keeper_error_event
        upload_to_gcs
    fi
    ;;
6*)
    bag_gen_folder localization_csv
    time_select
    mkdir -p /data/code/all_ws/ws/logpush_tmp/${UPLOAD_LOG_NAME}/localization_csv
    localization_csv_search landmark
    localization_csv_search localization2
    compress_only
#  if [[ "${HOSTNAME}" =~ ^wh.* ]] || [[ "${HOSTNAME}" =~ ^dl.* ]] || [[ "${HOSTNAME}" =~ ^eh.* ]] || [[ "${HOSTNAME}" =~ ^pd.* ]] || [[ "${HOSTNAME}" =~ ^hd.* ]] || [[ "${HOSTNAME}" =~ ^qz.* ]] || [[ "${HOSTNAME}" =~ ^tug.* ]] || [[ "${HOSTNAME}" =~ ^hkg.* ]] || [[ "${HOSTNAME}" =~ ^lcb.* ]] || [[ "${HOSTNAME}" =~ ^skq.* ]]; then
#       upload_to_nas
#    else
#        upload_to_gcs
#    fi
   ;;
7*)
    bag_gen_folder localization_csv
    mkdir -p /data/code/all_ws/ws/logpush_tmp/${UPLOAD_LOG_NAME}/localization_csv
    lcb_localization_csv landmark
    lcb_localization_csv localization2
    compress_only
;;
8*)
    bag_gen_folder supervisor_log
    mkdir -p /data/code/all_ws/ws/logpush_tmp/${UPLOAD_LOG_NAME}/supervisor_log
    log_search_copy_pre
    lcb_supervisor_log 
    lcb_supervisor_csv
    log_search_copy_post
    compress_only
;;
esac

